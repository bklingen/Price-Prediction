---
title: "Ridge"
author: "Thomas FitzGerald and Mei Maddox"
date: '2022-10-13'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
options(knitr.duplicate.label = 'allow')
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, message = FALSE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70))
```

```{r read.previous, echo=FALSE}
source(knitr::purl("../KaggleHousePrices.Rmd", quiet = TRUE))
```

```{r libraries, include=FALSE}
library(splines)
library(glmnet)
```


```{r}
fit.all <- train %>% select(-Id) %>% 
  lm(log(SalePrice) ~ ., data=.)
```


```{r}
X <- model.matrix(fit.all)[,-1]
X.sd = scale(X)
head(X.sd)
```

```{r}
fit.ridge <- glmnet(x=X.sd, y=train$SalePrice, alpha=0)
plot(fit.ridge, xvar="lambda", label=TRUE)
```

```{r}
cross.val <- cv.glmnet(x=X.sd, y=train$SalePrice, alpha=0)
plot(cross.val)
```


```{r}
fit.ridge.opt <- glmnet(x=X, y=train$SalePrice, 
                        lambda=cross.val$lambda.min, 
                        alpha = 1, standardize = TRUE)
coef(fit.ridge.opt)
```


`predict(fit.ridge.opt, newx=test)` raises an error regarding the number of variables. This is because the fit function creates dummy variables. We must model the test set to have the same structure as the modeled training set (175 variables).

```{r}
predicted.ridge = test %>% 
  select(-Id) %>% 
  model.matrix(as.formula("~."), .) %>%
  tibble::as_tibble() %>%
  select(., -setdiff(colnames(.), colnames(X))) %>%
  as.matrix() %>%
  predict(fit.ridge.opt, newx=.) %>% exp()
SubmitDF <- data.frame(test$Id, predicted.ridge)
colnames(SubmitDF) <- c("Id", "SalePrice")
write.csv(file='../submissions/ridge1.csv', SubmitDF, row.names = FALSE)
```



```{r}
ggplot(SubmitDF, aes(x=SalePrice)) + 
  geom_histogram(aes(y=..density..), 
                 colour="black", fill="white") +
  geom_density(alpha=.2, fill="#FF6666") +
  geom_density(mapping=aes(x=train$SalePrice[-1]), alpha=.2, fill="blue") +
  geom_vline(xintercept=max(train$SalePrice), color="blue", size=1, linetype="dashed")
```

## Influential Outliers

standardized residuals help determine regression outliers: 1171, 633, 524
```{r}
plot(fit.all, which=1)
```


```{r eval=FALSE}
fitted.values <- predict(fit.lasso.opt, newx=X)
residuals <- train$SalePrice - fitted.values
stand.resid <- scale(residuals)

outliers <- which(abs(stand.resid) > 3)
indexes <- 1:length(stand.resid)

ggplot(mapping=aes(x=fitted.values, y=stand.resid)) +
  geom_point(shape=1) +
  geom_hline(yintercept=0, color="red", size=1, linetype="dashed") +
  
  ggrepel::geom_text_repel(aes(label = ifelse(indexes %in% outliers, indexes, "")))
```


Hat-Values to identify influencial observations
```{r}
threshold <- 2*length(coefficients(fit.all))/nrow(fit.all$model)
plot(hatvalues(fit.all))
abline(h = threshold, lty = 2, col = "red") # add cutoff line
```

0.5: 524, 1171
1: 1299, 1424
```{r}
cooks.distance <- cooks.distance(fit.all)

plot(cooks.distance, main = "Cooks Distance for Influential Obs")
abline(h = 0.5, lty = 2, col = "orange") # add cutoff line
text(x=1:length(cooks.distance)+1, y=cooks.distance, labels=ifelse(cooks.distance>0.5, names(cooks.distance),""), col="orange")
abline(h = 1, lty = 2, col = "red") # add cutoff line
text(x=1:length(cooks.distance)+1, y=cooks.distance, labels=ifelse(cooks.distance>1, names(cooks.distance),""), col="red")  # add labels
```




```{r}
new.train <- train %>% filter(!(row_number() %in% c(524, 1171, 1299, 1424)))
fit.all <- new.train %>% select(-Id) %>%
  lm(log(SalePrice) ~ ., data=.)
```


```{r}
X <- model.matrix(fit.all)[,-1]
X.sd = scale(X)
head(X.sd)
```

```{r}
fit.ridge <- glmnet(x=X.sd, y=new.train$SalePrice, alpha=1)
plot(fit.ridge, xvar="lambda", label=TRUE)
```

```{r}
cross.val <- cv.glmnet(x=X.sd, y=new.train$SalePrice, alpha=1)
plot(cross.val)
```


```{r}
fit.ridge.opt <- glmnet(x=X, y=new.train$SalePrice, 
                        lambda=cross.val$lambda.min, 
                        alpha = 1, standardize = TRUE)
coef(fit.ridge.opt)
```

`predict(fit.lasso.opt, newx=test)` raises an error regarding the number of variables. This is because the fit function creates dummy variables. We must model the test set to have the same structure as the modeled training set (175 variables).

```{r}
predicted.ridge <- test %>% 
  select(-Id) %>% 
  model.matrix(as.formula("~."), .) %>%
  tibble::as_tibble() %>%
  select(., -setdiff(colnames(.), colnames(X))) %>%
  as.matrix() %>%
  predict(fit.ridge.opt, newx=.)
SubmitDF <- data.frame(test$Id, predicted.ridge)
colnames(SubmitDF) <- c("Id", "SalePrice")
write.csv(file='../submissions/ridge2.csv', SubmitDF, row.names = FALSE)
```


