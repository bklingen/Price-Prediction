---
title: "Predicting House Prices"
output:
  pdf_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kaggle Competition
[This Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) is about predicting house prices based on a set of around 80 predictor variables. Please read the brief description of the project and get familiar with the various predictors. We will have to do some initial cleaning to successfully work with these data. Overall, we (in teams) will use the provided training dataset to built a multiple linear regression model for predicting house prices. Once we have settled on a final model, we will use it with the predictors available in the testing dataset to predict house prices. The goal of the competition mentions that our predictions $\hat{y}_i$ for the houses in the testing data are compared to the (withheld) true selling prices $y_i^\text{test}$ via $\sum_i(\log \hat{y}_i - \log y_i^\text{test})^2$. Because selling prices are typically right-skewed, I think as a first step we will log-transform the selling prices of the houses in the training data to obtain a more bell-shaped distribution. However, although we will built a model for the log-prices, we will still have to submit the price of a house (and not the log-price) to Kaggle, together with the ID of the house.

## Loading and inspecting the train and test datasets

```{r}
library(tidyverse)
library(tidyr)
## Load Training Data
path_traindata <- 'https://raw.githubusercontent.com/bklingen/Price-Prediction/main/train.csv'
train <- read_csv(path_traindata)
dim(train)
## Load Testing Data
path_testdata <- 'https://raw.githubusercontent.com/bklingen/Price-Prediction/main/test.csv'
test <- read_csv(path_testdata)
dim(test)
```

This makes sense: We have one less column in test data because of the missing house prices.

But, are the column names the same? Let's find the "difference" between two sets: All the column names that are in the test data but not in the train data: 
```{r}
setdiff(colnames(test), colnames(train))
```
OK, good, and now the other way around:
```{r}
setdiff(colnames(train), colnames(test))
```
OK, great. So no surprises there. All predictors that exist in the train data set also appear in the test dataset.

Let's see how many quantitative and how many categorical predictors we have in the training dataset, at least at face value:
```{r}
train_quantPredictors = train %>% select(where(is.numeric)) %>% select(-SalePrice) 
train_catPredictors = train %>% select(where(is.character))
dim(train_quantPredictors)
dim(train_catPredictors)
```
Let's transform the categorical predictors into factors, which should make it easier to combine categories, create a category like "other", etc. 
```{r}
train_catPredictors = train_catPredictors %>% transmute_all(as.factor) 
```

First, let's see the category names and frequency for each variable:
```{r}
for(i in 1:ncol(train_catPredictors)) {
  print(colnames(train_catPredictors)[i])
  print("----")
  print(as.data.frame(fct_count(unlist(train_catPredictors[,i]))))
  print("--------------")
}
```

## Handle Numerical Features
### Marina: YearBuilt, GarageYrBlt 
Having a look at the data, I had the feeling that YearBuilt and GarageYrBlt would be quite correlated, because a garage is usually built at the same time as the house itself. Let's check:

```{r}
#First, check missing values in train and test set

#Null values in YearBuilt column
sum(is.na(train$YearBuilt))
sum(is.na(test$YearBuilt))
```
No missing values in YearBuilt column

```{r}

#Null values in GarageYrBlt column
sum(is.na(train$GarageYrBlt))
sum(is.na(test$GarageYrBlt))

```
We have some missing values in GarageYrBlt column in both the train and the test set. 
Since we want to check the correlation with another feature, we don't want to impute values or remove rows. By now we are just going to create a temporary dataframe that does not include the rows with missing values in GarageYrBlt column
```{r}
# Make a temporary dataframe without the rows where GarageYrBlt column in NAN
train_temp = train %>% drop_na("GarageYrBlt")
test_temp = test %>% drop_na("GarageYrBlt")
```


```{r}
#Check that we dont for missing values to make sure we got rid of them
sum(is.na(train_temp$GarageYrBlt))
sum(is.na(test_temp$GarageYrBlt))

```

Now we don't have NaNs, we can check the correlation between YearBuilt, GarageYrBlt
```{r}
# Chekcing correlations with GarageYrBlt
cor(train_temp['GarageYrBlt'], train_temp['YearBuilt'])
cor(test_temp['GarageYrBlt'], test_temp['YearBuilt'])
```

As expected, these two columns are quite correlated. Since GarageYrBlt has NaN values and YearBuilt has all the data, we are droping GarageYrBlt from the original dataframes.

```{r}
train = select(train, -c(GarageYrBlt))
test = select(test, -c(GarageYrBlt))
```

### Marina: GarageCars, GarageArea
Let's do the same with GarageCars, GarageArea which seem to be correlated.
```{r}
#First, check missing values in train and test set

#Null values in GarageCars column
sum(is.na(train$GarageCars))
sum(is.na(test$GarageCars))
```
```{r}
#Null values in GarageArea column
sum(is.na(train$GarageArea))
sum(is.na(test$GarageArea))
```
We have one missing values in GarageCars and GarageArea columns in the test set. 
Since we want to check the correlation with another feature, we don't want to impute values or remove rows. By now we are just going to create a temporary dataframe that does not include the rows with missing values in GarageCars and GarageArea columns
```{r}
# Make a temporary dataframe without the rows where GarageCars and GarageArea column in NAN
test_temp = test %>% drop_na("GarageCars", "GarageArea")
sum(is.na(test_temp$GarageCars))
sum(is.na(test_temp$GarageArea))
```

Now we don't have NaNs, we can check the correlation between GarageCars and GarageArea
```{r}
# Chekcing correlation between GarageCars and GarageArea
cor(train['GarageCars'], train['GarageArea'])
cor(test_temp['GarageCars'], test_temp['GarageArea'])
```

As expected, these two columns are quite correlated, so we are droping GarageCars (which is lees descriptive) from the original dataframes.
```{r}
train = select(train, -c(GarageCars))
test = select(test, -c(GarageCars))
```


## Handle Categorical Features
<<<<<<< HEAD


# Richard's Features

### RoofStyle

**combine flat, shed as other; gambrel, mansard, gable as gable; leave others as is**

```{r}
roof_price <- train %>% group_by(RoofStyle) %>% summarize(count=n(),
  mean(SalePrice), sd(SalePrice))

roof_price
```
```{r}
train$RoofStyle <- fct_collapse(train$RoofStyle, Other = c("Flat", "Shed"))
train$RoofStyle <- fct_collapse(train$RoofStyle, Gable = c("Gable", "Gambrel", "Mansard"))
```



### BldgType
**combine 2FmCon, Duplex as multifamily; leave others as is**

```{r}
bldg_price <- train %>% group_by(BldgType) %>% summarize(count=n(),
  mean(SalePrice), sd(SalePrice))

bldg_price
```
```{r}
train$BldgType <- fct_collapse(train$BldgType, MultiFam = c("2fmCon", "Duplex"))
```

### HouseStyle
**combine 1.5Fin, 1Story, split foyer, split level as less than 2 story; 2.5fin, 2Story as two story or greater; leave 1.5Unf and 2.5Unf as is since they drag down property values**

```{r}
style_price <- train %>% group_by(HouseStyle) %>% summarize(count=n(),
  mean(SalePrice), sd(SalePrice))

style_price 
```

```{r}
train$HouseStyle <- fct_collapse(train$HouseStyle, Less2story = c("1Story", "1.5Fin", "SFoyer", "SLvl"))

train$HouseStyle <- fct_collapse(train$HouseStyle, EqMore2story = c("2Story", "2.5Fin"))
=======
```{r}
cleanpool <- as.character(train_catPredictors$PoolQC)
cleanpool[is.na(cleanpool)] <- "none"
cleanpool <- as.factor(cleanpool)
```

```{r}
cleanfence <- as.character(train_catPredictors$Fence)
cleanfence[is.na(cleanfence)] <- "none"
cleanfence <- as.factor(cleanfence)
```

```{r}
cleanfunc <- as.character(train_catPredictors$Functional)
cleanfunc[cleanfunc == 'Min1' | cleanfunc == 'Min2'] <- "Minor"
cleanfunc[cleanfunc == 'Maj1' | cleanfunc == 'Maj2'] <- "Major"
cleanfunc[cleanfunc == 'Sev' | cleanfunc == 'Sal'] <- "Severe"
cleanfunc <- as.factor(cleanfunc)
```

```{r}
train_catPredictors$PoolQC <- cleanpool
train_catPredictors$Fence <- cleanfence
train_catPredictors$Functional <- cleanfunc
```

### Mileva: Heating, Electrical, FireplaceQu, HeatingQC, CentralAir
The processing for the Heating, Electrical, and FireplaceQu predictors is below. The HeatingQC and CentralAir predictors did not require any additional processing. 

```{r}
# Heating: Collapsed categores with low frequencies into "other"
heating <- as.factor(train_catPredictors$Heating)
heating <- fct_other(heating, keep=c("GasA", "GasW"))
train_catPredictors$Heating <- heating
```

```{r}
# Electrical: Collapsed similar categories together and handled missing values
electrical <- as.character(train_catPredictors$Electrical)

electrical <- fct_collapse(electrical, Fuse=c("FuseA", "FuseF", "FuseP"))
electrical <- fct_collapse(electrical, Other=c("Mix"))
electrical[is.na(electrical)] <- "Other"

train_catPredictors$Electrical <- electrical
```

```{r}
# Fireplace: Handled missing values
fireplace <- as.character(train_catPredictors$FireplaceQu)
fireplace[is.na(fireplace)] <- "none"
train_catPredictors$FireplaceQu <- as.factor(fireplace)
>>>>>>> 744290fa0e3268f7a85360fd0e4e3000e675090c
```

